{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a913c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- (1) 환자 환경 정의 --- #\n",
    "class PatientEnv:\n",
    "    def __init__(self, patient_data, max_tests=5):\n",
    "        self.patient_data = patient_data  # dict: {patient_id: {features, label}}   #환자의 id, 식별번호와 그에 따른 데이터가 들어있다.\n",
    "        self.max_tests = max_tests     #이 수치보다 검사결과를 많이 이용하면 에피소드를 강제종료한다.\n",
    "        self.test_list = ['test1', 'test2', 'test3', 'test4', 'test5']      #가능한 검사들의 리스트\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.steps = 0\n",
    "        self.patient_id = random.choice(list(self.patient_data.keys()))       #임의로 환자 선택\n",
    "        self.patient = self.patient_data[self.patient_id]            \n",
    "        self.available_tests = set(self.test_list)                   #아직 수행하지 않아서 남아 있는 검사들의 세트\n",
    "        self.observed = []\n",
    "        self.state = np.zeros(len(self.test_list))  # 초기 상태는 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):   #action: 수행하고자 하는 테스트의 이름 인덱스\n",
    "        if action < len(self.test_list):  # 검사 선택\n",
    "            test_name = self.test_list[action]\n",
    "            if test_name not in self.available_tests:       #수행하라고 action에 들어있는 테스트가 이미 실행한 것이면:  현재상태 반환, 페널티 주고, 테스트가 안 끝났다고 말한다.\n",
    "                return self.state, -5.0, False  # 중복 검사 penalty\n",
    "            self.available_tests.remove(test_name)            #검사 선택 -> 가능한 검사목록에서 제거, \n",
    "            self.state[action] = self.patient['features'][action]  # 검사 결과 반영해 state에 해당 검사인덱스에 대한 결과를 기록한다.\n",
    "            reward = -1.0  # 검사 비용\n",
    "            self.done = False\n",
    "        else:  # action값이 크면 진단 시도 -> label값과 비교해서 같은지 확인 \n",
    "            pred = action - len(self.test_list)  # 진단 클래스 index\n",
    "            correct = int(pred == self.patient['label'])                  #정확도기준으로는 못하기 때문에 new_dx와의 값 차이에 비례해서 페널티 주도록 하자!\n",
    "            reward = 100.0 if correct else -100.0\n",
    "            self.done = True\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_tests:\n",
    "            self.done = True\n",
    "        return self.state.copy(), reward, self.done\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return len(self.test_list) + len(set(p['label'] for p in self.patient_data.values()))    #존재하는 검사의 개수 + 가능한 new_dx의 가짓수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcfb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (2) 정책 & 가치 모델 정의 --- #\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.value = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy(x), self.value(x)    #들어온 state값에 대해서 policy 결과는 상태가치함수결과를 반환 -> 가능한 action별 확률을 출력. value 결과는 state에 대한 가치함수결과 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d24e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, clip_eps=0.2, gamma=0.99, lr=3e-4):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.clip_eps = clip_eps\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs, _ = self.model(state)      #가치신경망 -> 정책신경망을 state가 통과 => probs에는 정책, 즉 행동별 확률이 기록된다.\n",
    "        dist = torch.distributions.Categorical(probs)        #dist는 probs 확률에 따라 행동을 선택하는 다항분포객체이다.\n",
    "        action = dist.sample()                               #dist에 따라 행동선택\n",
    "        return action.item(), dist.log_prob(action)        #선택한 행동의 인덱스와 그 행동의 확률의 로그값 반환\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):      #가장 최근에 종료된 때의 보상부터 시작해서 앞의 보상들을 더하고 전체보상에 감마를 곱해가면서 누적 보상을 얻는다.\n",
    "            R = r + self.gamma * R * (1 - d)               \n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states = torch.FloatTensor([t[0] for t in trajectories])\n",
    "        actions = torch.LongTensor([t[1] for t in trajectories]).unsqueeze(1)\n",
    "        old_log_probs = torch.cat([t[2] for t in trajectories]).detach()\n",
    "        returns = torch.FloatTensor(self.compute_returns(        #rewards, dones 이용해 누적 reward, return 계산한다.\n",
    "            [t[3] for t in trajectories],\n",
    "            [t[4] for t in trajectories]\n",
    "        )).unsqueeze(1)\n",
    "\n",
    "        probs, values = self.model(states)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        log_probs = dist.log_prob(actions.squeeze())\n",
    "\n",
    "        advantages = returns - values.detach()       #A=R-V\n",
    "\n",
    "        ratio = torch.exp(log_probs - old_log_probs)     #model을 통해 얻은 next pi / 이전 시기의 pi\n",
    "        clip = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps)     #클리핑으로 ratio 값 자른다.\n",
    "        policy_loss = -torch.min(ratio * advantages, clip * advantages).mean()     #RA, CA의 최솟값을 구하고 -붙여서 Gradient Ascent하게 해 훈련시킨다. \n",
    "        value_loss = nn.MSELoss()(values, returns) #\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 환자 데이터 생성 (예: 100명, 각 5개 feature, 3개 클래스)\n",
    "    patient_data = {\n",
    "        i: {\n",
    "            \"features\": np.random.rand(5),\n",
    "            \"label\": np.random.randint(0, 3)\n",
    "        }\n",
    "        for i in range(100)\n",
    "    }\n",
    "\n",
    "    env = PatientEnv(patient_data)\n",
    "    agent = PPOAgent(state_dim=5, action_dim=env.get_action_space())\n",
    "\n",
    "    for epoch in range(300):\n",
    "        trajectories = []\n",
    "        for _ in range(20):  # batch of episodes\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode = []\n",
    "            while not done:\n",
    "                action, log_prob = agent.get_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode.append((state, action, log_prob, reward, done))\n",
    "                state = next_state\n",
    "            trajectories.extend(episode)\n",
    "        agent.update(trajectories)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} 완료\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
